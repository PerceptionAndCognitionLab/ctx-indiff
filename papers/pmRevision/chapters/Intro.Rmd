---
title: "Individual differences, Introduction"
author: "Haaf & Rouder""
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---

```{r 'truncplot', child="../figures/truncplot.Rmd"}
```

```{r 'exampleplot', child="../figures/exampleplot.Rmd"}
```

Many experimental tasks in psychology have a certain character where participants perform many trials in a small number of conditions.  For example, in most social-cognitive priming tasks, participants perform hundreds of trials in the primed and unprimed conditions [@Amodio:etal:2004].  In reading tasks, participants may read hundreds of words [e.g. @Balota:etal:2004]; in perceptual tasks, participants may identify hundreds of items [e.g. @Swagman:etal:2015]; in memory tasks, participants may be asked to recognize hundreds of previously-studied memoranda.  Examples in decision making and attention spring immediately to mind. We may call such tasks *massively repeated*.

One of the common questions researchers ask in these tasks is whether there is an effect of a manipulation on an outcome variable.  For example, in a priming task, a researcher may ask if responses when primed are faster than those when not primed.  Likewise, massively repeated tasks may be used to ask whether the strength of a stimulus, the context in which it is presented, or the attention directed toward it affects the response.

If we limit our attention to tasks with two conditions, say as in the priming case, the usual course is to aggregate across the repetitions to produce a participant-by-condition mean score.  These mean scores may be subtracted across the conditions to produce a participant-specific observed effect, and these effects may be tested with a $t$-test.  If the $t$-test null is rejected, then the researcher may conclude that there is evidence for an average effect across the population of participants.

An advantage of this aggregate approach are its simplicity; it may be performed by virtually anyone. A limitation, however, is that researchers are unable to address individual variation in the participants.  A significant $t$-test does not guarantee that all individuals display a priming effect.  Some truly may while others truly may not.  Even more alarming, while some may truly have the usual priming effect, others may truly have the opposite, negative effect.   

Figure `r fig.trunc` shows two scenarios. Plotted are hypothetical distributions of individuals' true effects.  In the first scenario, displayed in  Figure `r fig.trunc`A, all of these true effects are positive, and they come from a truncated normal distribution. In the second scenario, displayed in  Figure `r fig.trunc`B, individuals' effects are not constrained to be positive, and, instead, follow the usual normal distribution. Importantly, a sizable minority of participants have truly negative effects.

We think the differences between these scenarios are theoretically important.  In the first scenario, `r fig.trunc`A, the mean effect is interpretable as a proxy for what all participants do. This commonality of direction may be used to specify common mechanisms, say that priming is automatic and beyond strategic control [@Greenwald:etal:1995], or that stimulus strength has a simple, common neurological correlate, say that more strength corresponds to greater neural firing rates in certain cell assemblies [@Roitman:Shadlen:2002].

In the second scenario, `r fig.trunc`B, effects are mixed in direction.  A real-world example is handedness.  Imagine a task where people throw balls with their right and left hands, and we ask for which hand did the ball go further.  Here, right-handed people are almost always stronger with their right hand; left-handed people tend to be stronger with their left hand. In the population, more individuals are right-handed. So on average, people may be stronger with their right hand. But this average strength does not convey any meaningfull information about all individuals in the sample. Thus, this case corresponds to more complex theoretical implications that assume more than one underlying mechanism. Indeed, handedness is complicated, and it is better described as a syndrome than a single phenomenon [@Coren:1993].  Other possible examples of complicated phenomena from the literature are different routes of attitude formation [@sweldens2014role] or different stategies in decision theory [@Kahneman:Tversky:1972].

Unfortunately, it is impossible to tell which scenario holds and which theories are implicated from a $t$-test. To see the problem, consider Figure `r fig.trunc`C and D.  The vertical lines in Figure `r fig.trunc`C and D denote the mean of the distributions, and the dashed line shows the distributions of sample means. These distributions are the basis for the $t$-test, and the fact that these are identical illustrates the problem.

Assessing whether true effects are all in one direction may seem simple, but the presence of sampling noise makes it quite hard.  For an illustration of this difficulty, consider the following Stroop interference experiment from @vonBastian:etal:2015.  In the classic Stroop interference task [@Stroop:1935], participants are asked to name the color of displayed words.  The words themselves are color names, such as RED and GREEN, and the meaning of the word may be congruent with the color, i.e., RED displayed in red, or incongruent with the color, i.e., GREEN displayed in red.  Stroop interference refers to the slowdown of color identification when the meaning is incongruent, and it is a robust phenomenon [@MacLeod:1991]. The question at hand is not the existence of Stroop interference, but whether all people display the effect in the usual direction.

Figure `r fig.example` shows the ordered observed Stroop interference effects, $d_i$, for 121 individuals from @vonBastian:etal:2015. People vary considerably in the size of their effects: they range from `r round(min(diff)*1000)` ms to `r round(max(diff)*1000)` ms.  The problem is that it is difficult to distinguish between noise variation and true variation from the observed values. The 95% confidence intervals around invidivduals' sample effects are plotted, and these, unfortunately, do not provide any direct way of assessing whether all effects are positive.  CIs are about each individual in isolation and cannot be used to answer questions about a group of individuals.  We note that no CI in the figure is located exclusively below zero.  Even though no specific individual is identified as definitively negative, there may be enough evidence across the collection of individuals to state that some of these individuals have true negative effects without identifying which one.

The problem at hand is known as order-restricted inference, and there is a voluminous frequentist literature on it [see @Robertson:etal:1988].  Order-restricted inference is straightforward for one-dimensional cases, say whether the grand mean is greater than zero or not.  The usual frequentist solution is to adjust rejection regions as is done in one-sided $t$-tests.  There are comparable implementations with Bayesian model comparison, where models on group means incorporate order restrictions [@Klugkist:Hoijtink:2007]. Klugkist and colleagues introduced a Bayesian approach to order-restricted inference in ANOVA and ANCOVA [@Klugkist:etal:2005], and repeated measure settings [@Mulder:etal:2009]. But the question here is whether a separate order-restriction holds simultaneously *for all individuals*.  Therefore, we must assess 121 order restrictions simultaneously, and this problem is complicated.  Of note, usual AIC and BIC approaches are not applicable because penalties reflect the number of parameters rather than their direction [@Klugkist:Hoijtink:2007].

Our approach is to analyze Bayesian mixed models with order and equality constraints on individuals.[^r]  There are two advantages of Bayesian methods in this context:  1.  It is tractable.  Bayesian analysis is conceptually straightforward and computationally feasible.  2. Bayesian methods, particularly Bayes factors, offer a calibration for inference that is appropriate for order constraints [see also @Klugkist:Hoijtink:2007].

[^r]: All analyses were conducted using `r my_citation`.

Before describing the models we developed to assess constraints like the one in Figure `r fig.trunc`A, it is worthwhile to ask if these constraints are useful.  We consider two critiques.  The first is that the order constraints are so natural and obvious that they assuredly hold *a priori*.  For example, it is hard to imagine that anyone identifies dim flashes of light more quickly than bright flashes, that anyone reads long novel nonwords faster than short novel nonwords, or that anyone forgets repeated items at a greater rate than unrepeated items.  Yet, we can think of examples where such restrictions do not hold such as the handedness example provided previously.  A second critique is the diametric opposite of the above critique.  It is that these order constraints can never hold exactly.  There must be somebody, somewhere who has a negative true Stroop effect.  This critique reminds us of the one that the null is never true [@Cohen:1994], for which there are several salient rebuttals [@Rouder:Morey:2012].  We consider statements like "Everyone Stroops" to be of high value even if they hold approximately or platonically.  If broadly applicable in common settings, they serve as important statements of constraint on theory.  In summary, we consider the two extreme positions---that order constraints always hold or that order constraint never hold---to be intellectually unsatisfying.  The best course then is to assess the evidence from data for these constraints.
