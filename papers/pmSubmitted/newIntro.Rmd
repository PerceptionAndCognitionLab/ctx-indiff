---
title: "Individual differences, Introduction"
author: "Haaf & Rouder""
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---

#Introduction

Many experimental tasks in psychology have a massively repeated character in which participants perform a great many trials in a small number of conditions.  For example, in most social-cognitive priming tasks, participants perform hundreds of trials in the primed and unprimed conditions Amodio:etal:2004.  In reading tasks, participants may read hundreds of words e.g., Balota & Yap; in perceptual tasks, participants may identify hundreds of items e.g., Swagman et:2015 in memory tasks, participants may be asked to recognize hundreds of previous-studied memoranda citation.  Other examples in decision making and attention spring immediately to mind as well.

One of the common questions researchers ask in massively repeated tasks is whether there is an effect of a targeted manipulation on an outcome variable.  For example, in the priming tasks, a researcher may ask if responses when primed are faster than those when not primed.  Likewise, massively repeated tasks may be used to ask whether the strength of a stimulus, the context in which it is presented, or the attention directed toward it affect the response.

If we limit our attention to two conditions, say as in the priming case, the usual course is to aggregate across the repetitions to produce a participant-by-condition mean score.  These mean scores may be subtracted across the conditions to produce a participant-specific observed effects, and these effects may be tested with a $t$-test.  If the t-test null is rejected, then the researcher may conclude that there is an average effect across the population.

The great value of this aggregate approach is that it is simple and can be performed by all researchers, including undergraduate and graduate students.  The limitation, however, is that the researcher is unable to address individual variation.  A significant $t$-test does not guarantee that all individuals display a priming effect.  Some might while others might not.  Even more alarming, while some might have the usual priming effect, while others may have the opposite negative effect.   Figure `r fig.trunc` shows two scenarios that yield identical t-tests.  In the first scenario, true individual priming effects follow a normal, and there is sufficient variability that there are negative true effects for a minority of participants.  In the second scenario, all participants are constrained to have a positive effect.  

We think the differences between these scenarios are theoretically important in massively repeated designs.  We distinguish between three cases: In the first case all participants have the same direction of their true effect (Figure) .  In this case, the mean effect is interpretable as a proxy for what all participants do. This commonality of direction may be used to specify common mechanisms, say that priming is automatic and beyond strategic control (Greenwald et al), or that stimulus strength has a simple, common neurological correlate, say that more strength corresponds to greater neural firing rates in certain cell assemblies (Shadlen & Reitman, 2003).  In the second case some participants have the usual true effect while others have the blah blah blah


Assessing whether true effects are all in one direction or not may seem simple, but in practice, it is quite hard.    For an illustration of this difficulty, consider Figure `r fig.example`.  This figure shows Stroop effects across 121 participants.  In the Stroop talk, 

The problem in Figure~  is that it is difficult to distinguish between noise variation and true variation from the observed values. Furthermore, the 95% confidence intervals around invidivdual's sample effects, do not provide any direct way of assessing whether all effects are positive.  We note that no CI in the figure is located exclusively below zero.  Yet, CIs are about each individual in isolation and cannot be used to answer questions about a group of individuals.  For instance, it could be that some fraction of individuals have negative sample effects with CIs that just cover zero.  In this case, even though no specific individual is identified as definitively negative, there may be enough evidence across the collection of individuals to state that some of these individuals have true negative effects without identifying which one.

The problem at hand is known as order-restricted inference, and there is a voluminous frequentist literature on it [see @Robertson:etal:1988].  Order restricted inference is straightforward for one-dimensional cases, say whether the grand mean is greater than zero or not.  The usual solution is to adjust rejection regions as is done in one-sided $t$-tests.   But the question at hand is whether a separate order-restriction holds simultaneously for all individuals.  Therefore, we must assess 121 order restrictions simultaneously, and this is a much more complicated problem.  Of note, usual AIC and BIC approaches are not applicable because penalties reflect the number of parameters rather than their direction [@Klugkist:Hoijtink:2007].

Our approach to order-restricted inference is to analyze Bayesian mixed models with order and equality constraints.[^r]  There are two advantages of Bayesian methods in this context:  1.  It is tractable.  Bayesian analysis is conceptually straightforward and computationally feasible.  2. Bayesian methods, particularly Bayes factors, offer a calibration for inference that is situated for order constraints [see also @Klugkist:Hoijtink:2007].

[^r]: All analyses were conducted using `r my_citation`.

Before describing the models we developed to assess constraints like the one in right panel of Figure `r fig.trunc`, it is worthwhile to ask if these constraints are useful.  We consider two critiques.  The first is that the inequality constraints are so natural and obvious that they assuredly hold *a priori*.  For example, it is hard to imagine that anyone identifies dim flashes of light more quickly than bright flashes, that anyone reads long novel nonwords faster than short novel nonwords, or that anyone forgets repeated items at a greater rate than unrepeated items.  Yet, we can think of examples where they do not hold.  Consider the task of throwing a ball with ones left hand and right hand.  Many people will throw it farther with their right hand, and we can call this a positive right-hand effect.  Some, however, will have a negative right-hand effect, that is, they will throw the ball farther with their left hand.  Hence, positive effects are not always expected, and searching for them is a valuable way of providing theoretical constraint.  

A second critique is the diametric opposite of the above critique.  It is that these inequality constraints can never hold exactly.  There must be somebody, somewhere who has a negative true Stroop effect.  This critique reminds us of the one that the null is never true [@Cohen:1994], for which there are several salient rebuttals [@Rouder:Morey:2012].  We consider statements like "Everyone Stroops" to be incredibly valuable even if they hold approximately or platonically.  If broadly applicable in common settings, they still serve as important statements of constraint on theory and the search for them remains important and topical.  In summary, we consider the positions that the inequality constraint holds automatically or never holds to be unsatisfying.  The best course then is to assess the evidence from data for these constraints.
