---
title: "Individual differences, Models"
author: "Haaf & Rouder""
output:
  pdf_document: papaja::apa6_pdf
  html_document: default
  word_document: papaja::apa6_word
---

```{r 'modelplot', child="../figures/modelplot.Rmd"}
```

We explicitly consider experiments with two conditions, which are generically termed *control* and *treatment* here.  To model individual variability in massively repeated designs, we adopt the following notation:  Let $Y_{ijk}$ be a response variable, say response time (RT), for the $k$th replicate for the $i$th participant, $i=1,\ldots,I$ in the $j$th condition, $j=1,2$ with $k=1,\ldots,K_{ij}$.  We place a linear model on $Y_{ijk}$:
\begin{equation} \label{base1}
Y_{ijk} \stackrel{iid}{\sim} \mbox{Normal}\left([\mu+\alpha_i]+x_j[\theta_i],\sigma^2\right).
\end{equation}
Here, the term $(\mu+\alpha_i)$ serve as intercepts with $\mu$ being the grand mean of intercepts and $\alpha_i$ being individual deviations.  The term $x_j$ codes the condition, with $x_j=0$ for the control condition and $x_j=1$ for the treatment condition.  The effect is the slope, $\theta_i$, and the collection of these parameters across participants are the target of interest.

We develop a series of four mixed models on these effect parameters.  At one end of the spectrum, the most general model with the least constraint simply posits that the individual effects follow a graded distribution.  At the other end, the most constrained model specifies that there is no effect for every individual. The following models are ordered from the least constrained to the most, and the full set provides a useful tool for assessing the constraint in effects. We then use Bayesian model comparison, discussed subsequently, to assess what level of constraint is most appropriate for a set of data.

## The Unstructured Model
The unstructered model is denoted $\calM_u$ and places no order or equality constraints on the collection of effects: 

\[
  \begin{array}{llr}
\calM_u: && \theta_i \stackrel{iid}{\sim} \mbox{Normal}(\nu,\eta^2),\\
\end{array}
\]

where $\nu$  and $\eta^2$ are population-level parameters describing the mean and variance of effects across the population. This model is quite flexible in that all combinations of individual effects are plausible. Figure `r fig.modelplot`$\mbox{A}_1$ shows this flexiblity for two participants.  The true-effect value for the first individual is shown on the $x$-axis, the true-effect value for the second is shown on the $y$-axis.  As can be seen, combinations of effects in the same and opposite directions for the two individuals have plausibility.  Here, for illustration, $\nu$ is set to 0 ms and $\eta$ is set to 230 ms.  In application, $\nu$ and $\eta^2$ are treated as parameters that are free to vary as a function of data.

##The Positive-Effects Model
The positive-effects model, denoted $\calM_+$, captures the constraint that each individual has a true effect in the predicted direction.
\[
  \begin{array}{llr}
\calM_+: & & \theta_i \stackrel{iid}{\sim} \mbox{Normal}_+(\nu,\eta^2),\\
  \end{array}
\]
where $\mbox{Normal}_+$ denotes a truncated normal distribution with a lower bound at zero.  The probability density distribution of $\theta_i$ for two participants is shown in Figure `r fig.modelplot`$\mbox{B}_1$.  Note that the distribution in the figure is darker, that is more *dense*, than the distribution in Figure `r fig.modelplot`$\mbox{A}_1$. The reason for that is the reduction of space. The space of valid parameter values in this illustration is $\frac{1}{4}$th the space of valid parameter values in the unstructured model as the positive-effect model occupies only 1 of the 4 quandrants.  This reduction of space relative to the unstructued model becomes larger as additional participants are considered.

##The Common-Effect Model
The common-effect model, denoted $\calM_1$, captures the constraint that each individual has the same effect:  
\[
  \begin{array}{llrr}
\calM_1: & & \theta_i = \nu.\\
  \end{array}
\]

This common-effect model seems *a priori* unlikely as it posits a constant effect, for example a common priming effect, across all individuals.  Yet, we include it here for two reasons:  First, it is a logical null that can be used to benchmark claims of individual differences.  If this common-effects model provides a superior description and yet we view it as unlikely, we may then turn our attention to the adequacy of the design to capture individual differences.  In this regard, the model serves as a valuable design check. Second, we think commonalities should be given more *a priori* weight.  Important invariances, even if they hold approximately, serve as good structure to build theory.  Hence, if individual effect vary to a very small extend, $\calM_1$ could still be the preferred model.

For this model, $\calM_1$, the distribution of $\theta_i$ for two participants follows the diagonal as shown in Figure `r fig.modelplot`$\mbox{C}_1$. The geometry of the constraint is reducing a volume to a single line.

##The Null Model
The null model, denoted $\calM_0$, specifies that each participant's true effect is identically zero
\[
  \begin{array}{llr}
\calM_0: & & \theta_i = 0.\\
  \end{array}
\]
As everyone's effect is fixed to zero in this model, the distribution of $\theta_i$ for two participants shown in Figure `r fig.modelplot`$\mbox{D}_1$ is simply a point at zero. 

##Additional Specifications

We analyze the above four models in a Bayesian framework.  Additional specifications are needed for parameters $\mu$, $\sigma$, $\nu$, $\eta^2$, and the collection of $\alpha_i$.  Our choices are motivated by the following two considerations: First, we adopt specifications that lead to computationally convenient algorithms for model comparison.  Second, we adopt specifications that adhere to a subjective Bayesian philosophy [@DeGroot:1982; @Goldstein:2006; @Rouder:etal:2016b], where priors are weakly informative and reflect a reasonable range of beliefs.   Here, the prior settings reflect our *a priori* general substantive knowledge of generic data observed in these tasks. 

We adopt what is known as a Zellner $g$-prior specification [@Zellner:1986].  This specification is well studied and has been influential in linear modeling in statistics [e.g. @Bayarri:Garcia-Donato:2007; @Liang:etal:2008; @overstall2010default].  It underlies most Bayes factor development in psychology including development by Rouder, Morey, Wagenmakers and colleagues [@Rouder:etal:2009a; Rouder:etal:2012; @Rouder:Morey:2012; @Wetzels:Wagenmakers:2012].  In this context, we follow the ANOVA development by @Rouder:etal:2012 as follows:

Consider the following $g$-prior specification on $\alpha_i$:
\begin{equation} \label{alpha}
\alpha_i | g_{\alpha},\sigma^2 \stackrel{iid}{\sim} \mbox{Normal}(0, \sigma^2 g_{\alpha}).
\end{equation}
In the g-prior specification the prior on effect parameters, in this case $\alpha_i$, is a function of $\sigma^2$.  The role of $\sigma^2$ in the prior may not be transparent.  It is helpful to present an equivalent specificiation in *standardized effect sizes* rather than in effects.  Let $\alpha_i^*$ be the $i$th individual's intercept effect size, where $\alpha_i^*=\alpha_i/\sigma$.  The model reparameterized in this effect-size parameter may be given as $Y_{ijk} \sim \mbox{Normal}([\mu+\sigma\alpha_i^*]+x_j\theta_i,\sigma^2)$, where the prior on $\alpha_i^* \sim \mbox{Normal}(0, g_{\alpha})$.  Here we see there is no mystery placing the $\sigma^2$ in the prior variance in (\ref{alpha}).  It allows us to focus on $g$, which is the prior variance on effect sizes.  Effect sizes are convenient because psychologists have developed extensive intuition about how these should vary across manipulations and domains.

In $g$-prior specifications, priors may be placed on the $g$ parameter.  This additional specification is less assumptive and represents more uncertainty for the analyst.  The usual specification [@Zellner:Siow:1980], which we follow, is a scaled inverse-$\chi^2$ with one degree-of-freedom.[^3]
\[
g_{\alpha} \sim \mbox{Inverse-$\chi^2$} (r_{\alpha}^2).
\]
The quantity $r_{\alpha}^2$ is a scaling setting on variability (in standardized units), and it must be set *a priori*.  

Figure `r fig.marginals`A helps provide guidance for setting $r_\alpha$.  It shows the marginal prior for two different individuals' $\alpha$ parameters.  The distribution is centered at zero (insuring that $\mu$ is the grand mean). The value of $r_\alpha$ sets the scale, and the plot shows the case for $r_\alpha=1$. We show the values in time units (ms) and in effect-size units relative to a value of $\sigma = 300$ ms.   We can use our substantive knowledge of response times to set $r_\alpha$.  In the analyzed tasks, response times are measured for the identification of simple stimulus properties such as the color of a word.   Response times in simple choice tasks tend to have standard deviations around  200 ms to 400 ms for repeated trials within a person [@Luce:1986].  The average, 300 ms,  serves as a ballpark expectation for $\sigma$.  People also tend to vary from one another by about the same amount of 300 ms.  For example, one person's mean might be 500 ms while another's might be 800 ms.  With this knowledge in mind, we set $r_{\alpha}$ to 1.0, encoding the believe that the variation of individual baselines has a characteristic scale of $\sigma$.

We use a similar rational and $g$-prior structure on parameter $\nu$:
\begin{align*}
\nu|g_{\nu} &\sim \mbox{Normal}(0,\sigma^2g_{\nu})\\
g_{\nu} &\sim \mbox{Inverse-$\chi^2$}(r^2_{\nu}).\\
\end{align*}

To complete the specification, we let $g_\theta = \eta^2/\sigma^2$, the standardized variance of the effects. As before, the prior on $g_\theta$ is
\[
g_\theta \sim \mbox{Inverse-$\chi^2$}(r^2_{\theta}).
\]
To set values of $r_\nu$, we note that average effects in tasks like these tend to be on the order of tens of milleseconds, and we use 50 ms, or 1/6th of $\sigma$, as the scale in the inverse-$\chi^2$ prior ($r^2_\nu = (1/6)^2$).  To set $r_\theta$ we consider individual variability around the mean effect.  It is difficult to know what the true variation of individual effects is because the observed variability is confounded by trial noise.  Our working assumption is that the scale should be also on the order of tens of milleseconds. Moreover we expect the variability to be no larger than the size of the average effect. Using this rational, we set the scale on the standard deviation of individual differences to be 30ms, or 1/10th of $\sigma$ ($r^2_\theta = (1/10)^2$).  

Figure `r fig.marginals`B shows the marginal priors for two individuals' effects $\theta_i$. The correlation in the marginal priors arises from the hierarchical nature of the specification in the unstructued and positive models.  The variability in $\nu$ is shared and induces the correlation, and the degree of correlation is a function of the amount of shared variance, reflecting the setting of $r_\nu$, and the amound of unique variance, reflecting the setting of $r_\theta$.  

It is important to know how these substantive choices for setting $r_\alpha$, $r_\nu$, and $r_\theta$ affect model comparison results.  We visit this issue after presenting the results of the real-world application.


[^3]: The density of the inverse $\chi^2$ distribution is $f(\sigma^2;b)=\frac{\sqrt{b}}{\Gamma(1/2)}\left(\sigma^2\right)^{-3/2}\mbox{exp}\left(\frac{-b}{\sigma^2}\right)$, where $b$ is a  scale parameter.

[^4]: The density of $\eta$ is obtained by the usual transformation-of-variables techniques [@Hogg:Craig:1978], and is given by $g(\eta;c,d,e)=2\eta\times f(\eta^2;c,d)/(1-F(e^2;c,d))$ where $f$ and $F$ are the density and cumulative distribution functions of the inverse gamma, and $c$ and $d$ are shape and scale parameters on $\eta^2$, and $e^2$ is the lower limit of $\eta^2$.

