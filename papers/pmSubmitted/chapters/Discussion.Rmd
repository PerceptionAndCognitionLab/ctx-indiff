---
title: "General Discussion"
author: "Julia Haaf and Jeff Rouder"
date: "November 2, 2016"
output: html_document
---

In this paper we develop a set of Bayesian mixed models for assessing equality and order constraints in simple experimental paradigms.  These models range through four steps: the simplest null model, the model where all individual share a common effect, the model where individual effects vary but share a common direction, and an unstructured model that places no such constraints on individual variability.  The models developed here lead to computationally convenient Bayes factors, whoch serve as principled measures of the strength of evidence for the varying degrees of constraint in the data.

From a psychological perspective, perhaps the most important consideration for well-established effects is whether the order constraint holds.  In the Stroop case, for example, we ask whether all individuals have true Stroop effects in the same direction where congruent colors are named more quickly than incongruent ones. This constraint is compatible with the leading explanation that Stroop interference results from the fact that reading is quick and automatic for competent readers [@MacLeod:1991].  And assuming that all of the college participants are competent readers, then the constraint should hold.  Indeed, we suspect that many tasks will plausibly obey an order constraint. The interpretation is that they are mediated by nearly universal, automatic processes that do not admit a reverse ordering.  We do not expect, however, that all tasks will order as such, and in those that do not, it is reasonable to search for differing strategies and processing among participants.

## Stroop, Simon, and Eriksen Interference

In the course of development, we chose Stroop, Simon, and Flanker interference as our first application.  We are familiar with these types of interference [@Pratte:etal:2010a; @Rouder:etal:2010d; @Speckman:etal:2008] and were fairly certain *a priori* that Stroop intereference would obey the order constraint.  Hence, we are not at all surprised that in all three Stroop data sets, the positive-effects, or positive common-effect models were preferred.  

Stroop effects follow a common pattern when response time distributions are considered [@Rouder:etal:2010d].  Figure `r fig.delta` shows *delta plots* for Data Sets 1-6. A delta plot, first introduced by @DeJong:etal:1994, is a rotated version of a QQ plot [@Zhang:Kornblum:1997].  Each point denotes a RT percentile rank, and for the nine points displayed on each line, the ranks are the $10^{th},20^{th},\ldots,90^{th}$ percentiles.  The x-axis value of a point is the average RT at that percentile across the congruent and incongruent condition; the y-axis value is the difference, that is, the effect.[^delta]  There is a common pattern to these plots where effects tend to be smaller for lower percentiles (the faster responses) and larger for higher percentiles (the slower responses) [@Wagenmakers:Brown:2007; @Luce:1986; @Rouder:etal:2010d].  Indeed, Stroop interference effects in the literature show this common pattern [see @Pratte:etal:2010a], and it is present in the analyzed data sets as well (Figure `r fig.delta`A).

Simon interference has intrigued researchers for decades because the phenomenology seems idiosyncratic.  Delta plots commonly have a markedly different pattern.  They start positive but have a negative slope.  In several experimental reports, they actually cross the centerline implying that the slowest responses to incongruent stimuli are faster than the slowest responses to congruent ones!  The negative slope has been observed regularly [e.g. styrkowiec2013space; @Burle:etal:2005; @DeJong:etal:1994]. In Figure `r fig.delta`B, delta plots of the three Simon interference data sets are shown. Two of them show the typical negative-slope pattern. The reversal characterizes Data Set 5 as well as data from @Burle:etal:2005.  There is no other effect that we are aware of that has this negative-slope pattern.  Based on this result as well as electrophysiology results, the leading theory of Simon intereference is that it reflects a quick automatic process followed by a separate, slower, compensation process [@Ridderinkhof:1997].  

The plausible presence of two opposing processes for Simon interference sets up the possiblity that individual mean Simon effects are not order constrained.  Most participants may have a larger early automatic positive component, but some may have a larger, later negative component.  Hence, the unstructured model may provide the best account.

We considered predictions before analysis and speculated that Stroop interference would obey the order constraint---indeed, everyone Stroops.  We decided not to speculate about Simon interference beforehand.  If the order constraint held, it could be that the positive component was always larger than the negative one.  Likewise, if it did not, as it may not here, then there is individual variation in the relative sizes of the components.  

The combination of results do lead to a new conjecture.  There may be a profitable link between the delta plot pattern and the order restriction.  Perhaps whenever the slope is negative---an indication for two opposing processes---the order constraint may be violated.

The null Eriksen interference result is surprising because Eriksen effects are well known and often reported.  We did find a Eriksen effect in accuracy but, as shown, not in RT. Given the prevalence of RT effects in the literature, and that we did not collect these data, we do not further speculate on the null RT result.


## The Interpretability of a Common Effect
Two of our four models, the null model and the common effect model, are novel in that they specify no individual differences whatsoever. To our knowledge, individual differences researchers rarely consider a lack of individual differences.  In this regard, these models serve as important controls.  Indeed, in our data, these models with no individual differences outperformed the others in two of the seven data sets.  

The null model seems plausible or at least theoretically useful as a bound on human behavior.  There are simply some effects that do not occur for anyone, perhaps the ability to predict lottery outcomes above chance.  Hence, this model is fairly interpretable. But what about the common-effect model?  The notion that there is a natural constant for Stroop, that each person has the same exact effect, say 60 ms, is not too believable.  If we deem this model unbelievable, then how may we interpret the event that it outperforms the other models?

Perhaps the most fruitful position is to interpret the strong performance of this model in the context of the experimental design.  When the data are few in number, then simpler models are preferred to more complicated ones.  If researchers are interested in studying the structure of individual variation, then they need to employ designs with larger sample sizes.  The analyses here indicate that, from a design perspective, the number of trials per individual per condition is critical.  When this number is small, it is difficult to separate true individual variation from sample noise, and in light of this difficulity, the common-effect model provides a more parsimonious description (as it should).  As this number becomes larger, the separation of sources of variability is more stable, and evidence for true individual variation may be observed through model comparison.  Hence, we recommend the individual-difference researchers to consider the common-effect model as a check that designs have sufficient trials per individual to resolve true individual differences.

## Computational Considerations
The main computational issue in analyzing these models is computing Bayes factors.  The Bayes factor is the relative probability of data, and computing this probability requires accurate integration across all parameters with respect to the priors.  To make the integration convenient, we used a $g$-prior setup that is common in linear models [@Bayarri:Garcia-Donato:2007; @Liang:etal:2008].  In this setup, models are placed on standardized effect sizes rather than on effects themsleves.  With this setup, all parameters save the variabilities on effect sizes ($g_\alpha, g_\nu, g_\theta$) may be integrated in closed form, greatly simplifying the problem.

The $g$-prior setup was not our first choice.  Instead, we placed models on effects themselves, and tried a brute-force approach known as the *Savage-Dickey Density Ratio* [@dickey:1970] for computing the Bayes factor between the common-effect and unstructured models.  This method is precendented and recommended in psychology [@Wetzels:etal:2010; @Morey:etal:2011a; @Wagenmakers:etal:2010].  Unfortunately, the posterior density ratio estimates are too unstable to compute accurately with MCMC outputs.  Samples varied by some 40 orders of magnitude and convergence of the running mean was not achieved even with ten million MCMC iterations.  

## Limitations

Although the $g$-prior approach with symbolic integration of most parameters is suitable here, there are two substantial limitations affecting future generalizations.  One of our goals at the outset of this project was to include mixture models, say those where each individual had identically no effect or came from a positive distribution.  These types of mixture models are common in Bayesian analysis and go under the moniker of "spike-and-slab" priors [@George:McCulloch:1993].  The usual goal with these models is to categorize each individual as being in the spike, that is having no effect, or being in the slab, that is, having an effect.  And a separate Bayes factor is computed for each individual.  To our knowledge, there has been no consideration of the more natural goal in this setting---to assess whether the constraints from the mixture model, taken globally, provide a good description of the structure in the data.  We seek to compare the mixture model as a whole to the positive-effects model, the equivalent model without the spike. Whether the approach here generalizes to the mixture case remains unclear.

Another of our goals was developing realistic three-parameter lognormal models on response time such as those in [@Rouder:etal:2014b].  Currently, we use the normal with constant variance.  The lognormal is desirable because it captures the fact that response time distributions are skewed and that effects tend to be manifest in the scale rather than in the shift or shape of the distribution.  It is not clear, however, how to integrate log-normal parameters in the three-parameter version.  

Although these generalizations do not yield convenient closed form solutions for the integration, there are still avenues for future development.  Fortunately, the computational toolbox for Bayes factors in mixed models is sizable and growing.  Candidates for future work include bridge sampling [@meng:1996], importance sampling [@Kass:Raftery:1995] and Laplace approximation [@Kass:Raftery:1995; @Raftery:1996].  Hopefully, progress in computational issues will allow for useful generalizations of the models developed here.

[^delta]: Following @Zhang:Kornblum:1997, we compute the points on the delta plot lines as follows: Let $\bar{y}^p_j$ denote the average reaction time for the $p$th percentile and the $j$th condition across individuals and trials. This is a reasonable values as long as the assumption holds that the individuals' RT distributions do not vary in shape. We can now compute the average RT for the $p$th percentile, $\frac{\bar{y}^p_1 + \bar{y}^p_2}{2}$ and the average $p$th percentile effect, $\bar{y}^p_2 - \bar{y}^p_1$. For a delta plot, we plot those values against each other. If the slope of the line is positive, the fastest responses have the smallest, or even a negative effect. If the slope is negative, the fastest responses have the biggest effect.